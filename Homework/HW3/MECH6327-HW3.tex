\documentclass[letter]{article}
\renewcommand{\baselinestretch}{1.25}

\usepackage[margin=1in]{geometry}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{pythonhighlight}
\usepackage{hyperref}
\usepackage{fancyvrb}

% MATLAB Formating Code
\usepackage[numbered,framed]{matlab-prettifier}
\lstset{style=Matlab-editor,columns=fullflexible}
\renewcommand{\lstlistingname}{Script}
\newcommand{\scriptname}{\lstlistingname}

% Command for easier minimization problem def
\newcommand{\optpblm}[3][eq:default]{
	\begin{equation}\label{#1}
% Array method... more centered		
%		\begin{array}{rl}
%			\text{minimize}  \hspace{0.2in} &#2 \vspace{5pt}\\
%			\text{subject to} \hspace{0.2in} &#3
%		\end{array}
% Aligned method... left aligned... idk if its better
		\begin{aligned}
			\text{minimize} \hspace{0.5in} &#2\vspace{5pt}\\
			\text{subject to \hspace{0.5in}} &#3
		\end{aligned}	
	\end{equation}
}

\allowdisplaybreaks

\title{MECH 6327 - Homework 3}
\author{Jonas Wagner}
\date{2021, March 24}

\begin{document}

\maketitle

\newpage
\tableofcontents

\newpage
\section*{BV Textobook Problems}
\subsection{Problem 4.11}
\textbf{Problem:}
Formulate each problem as a LP and explained the relationship between the optimal solution of the problems and the solution of its LP.\\
\textbf{Solution:}
\subsubsection{Part a: Minimize $\norm{Ax-b}_\infty$}
Define the following minimization problem:
\optpblm{\norm{Ax-b}_\infty}{\text{math}}
From the definition of an $\infty$-norm as $$\norm{x}_\infty = \max_i \abs{x_i}$$ the following can be derived:
\optpblm{t}{\qty(Ax - b)_i\leq t, \ \forall i = 1,\dots,n\\
		- & \qty(Ax - b)_i\leq t, \ \forall i = 1,\dots,n}
Which is equivalent to the following linear program
\optpblm{t}{-\vb{1}t \leq Ax - b \leq \vb{1}t}
The resulted minimum to this equivalent problem, $t*$, is equivalent to the minimum of the original problem, $\norm{Ax^*-b}_\infty$.

From this the minimizing variable, $x^*$, can be found as: $$x^* = A^{-1} (\vb{1}^T t^* + b)$$

\newpage
\subsubsection{Part b: Minimize $\norm{Ax-b}_1$}
Define the following minimization problem:
\optpblm{\norm{Ax-b}_1}{\text{math}}
From the definition of an $1$-norm as $$\norm{x}_1 = \sum_i \abs{x_i}$$ the following can be derived:
\optpblm{t_1 + \cdots + t_n}{\qty(Ax - b)_i\leq t_i, \ \forall i = 1,\dots,n\\
-	& \qty(Ax - b)_i\leq t_i, \ \forall i = 1,\dots,n}
Which is equivalent to the following linear program
\optpblm{\vb{1}^T t}{-t \leq Ax - b \leq t}
The resulted minimum to this equivalent problem, $\vb{1}^T t$, is equivalent to the minimum of the original problem, $\norm{Ax-b}_1$.

From this the minimizing variable, $x^*$, can be found as: $$x^* = A^{-1} (t^* + b)$$

\newpage
\subsubsection{Part c: Minimize $\norm{Ax-b}_1$ subject to $\norm{x}_{\infty}\leq 1$}
Define the following minimization problem:
\optpblm{\norm{Ax-b}_1}{\norm{x}_\infty \leq 1}
From the definition of an $1$-norm as $$\norm{x}_1 = \sum_i \abs{x_i}$$ and the definition of an $\infty$-norm as $$\norm{x}_\infty = \max_i \abs{x_i}$$ the following can be derived:
\optpblm{t_1 + \cdots + t_n}{\qty(Ax - b)_i\leq t_i, \ \forall i = 1,\dots,n\\
	-& \qty(Ax - b)_i\leq t_i, \ \forall i = 1,\dots,n\\
	 & x_i \leq 1, \forall i = 1,\dots,n\\
	-& x_i \leq 1, \forall i = 1,\dots,n}
Which is equivalent to the following linear program
\optpblm{\vb{1}^T t}{-t \leq Ax - b \leq t\\
					&-\vb{1} \leq x \leq \vb{1}}

The resulted minimum to this equivalent problem, $\vb{1}^T t$, is equivalent to the minimum of the original problem, $\norm{Ax-b}_1$.

From this the minimizing variable, $x^*$, can be found as: $$x^* = A^{-1} (t^* + b)$$

\newpage
\subsubsection{Part d: Minimize $\norm{x}_1$ subject to $\norm{Ax-b}_\infty \leq 1$}
Define the following minimization problem:
\optpblm{\norm{x}_1}{\norm{Ax-b}_\infty \leq 1}
From the definition of an $1$-norm as $$\norm{x}_1 = \sum_i \abs{x_i}$$ and the definition of an $\infty$-norm as $$\norm{x}_\infty = \max_i \abs{x_i}$$ the following can be derived:
\optpblm{t_1 + \cdots + t_n}{x_i \leq t_i, \ \forall i = 1,\dots,n\\
							&-x_i \leq t_i, \ \forall i = 1,\dots,n\\
							&(Ax-b)_i \leq 1, \ \forall i = 1,\dots,n}
From this a linear program can be defined as:
\optpblm{\vb{1}^T t}{-t \leq x \leq t\\
			 		&Ax - b \leq \vb{1}}

The resulted minimum to this equivalent problem, $\vb{1}^T t$, is equivalent to the minimum of the original problem, $\norm{x}_1$.

From this the minimizing variable, $x^*$, can be found as: $$x^* = t^*$$

\newpage
\subsubsection{Part e: Minimize $\norm{Ax-b}_1 + \norm{x}_\infty$}
Define the following minimization problem:
\optpblm{\norm{Ax-b}_1 + \norm{x}_\infty}{math}
From the definition of an $1$-norm as $$\norm{x}_1 = \sum_i \abs{x_i}$$ and the definition of an $\infty$-norm as $$\norm{x}_\infty = \max_i \abs{x_i}$$ the following can be derived:
\optpblm{t_1 + \cdots + t_n + s}{
		\qty(Ax - b)_i\leq t_i, \ \forall i = 1,\dots,n\\
		-& \qty(Ax - b)_i\leq t_i, \ \forall i = 1,\dots,n\\
		&x_i \leq s, \ \forall i = 1,\dots,n\\
	-	&x_i \leq s, \ \forall i = 1,\dots,n}
This can be written as a standard linear program as:
\optpblm{\vb{1}^T t + s}{
		- t \leq Ax-b \leq t\\
		&-\vb{1}s \leq x \leq \vb{1}s}
The resulted minimum to this equivalent problem, $\vb{1}^T t + s$, is equivalent to the minimum of the original problem, $\norm{Ax-b}_1 + \norm{x}_\infty$. It should be noted that the $s$ and $\norm{x}_\infty$ are not used to find the minimization variable, but are important in weighting for solving for the minimization itself.

From this the minimizing variable, $x^*$, can be found as: $$x^* = A^{-1} (t^* + b)$$

\newpage
\subsection{Problem 4.16}
Consider the system given as
\begin{equation}\label{eq:dyn_sys_def}
	x(t+1) = A x(t) + b u(t), \ t = 0,\dots,N-1
\end{equation}
with $x(t) \in \real^n, u(t) \in \real, \forall t = 0,\dots,N-1$ and $A \in \real^{n\cross n}, b \in \real^n$, and $x(0) = 0$.\\

The minimum fuel optimal control problem is to select the minimum amount of inputs to minimize the amount of fuel used, given as
\begin{equation}\label{eq:min_fuel_problem_def}
	\begin{aligned}
		\text{minimize} \hspace{0.5in} &F = \sum_{t=1}^{N-1} f(u(t))\\
		\text{subject to \hspace{0.5in}} & x(t+1) = A x(t) + b u(t), \ t = 0,\dots,N-1\\
		& x(N) = x_{des}
	\end{aligned}	
\end{equation}
with $N$ as the time-horizon, $x_{des} \in \real^n$ as the desired final state, and $f: \real \to \real$ given as
\begin{equation}\label{eq:fuel_usage_def}
	f(a) = 
	\begin{cases}
		\abs{a} & \abs{a} \leq 1 \\
		2 \abs{a} - 1 & \abs{a} > 1
	\end{cases}
\end{equation}

\textbf{Problem:}
Formulate this problem as a Linear Program.\\

\textbf{Solution:}
First, \ref{eq:min_fuel_problem_def} can be rewritten in an epigraph form (with the additional assumption that $f(u(t))$ is always positive):
\optpblm[eq:min_fuel_problem_epigraph]{F_1 + \cdots + F_{N-1}}{
		f(u(t)) = F_t, \ \forall t = 1, \dots, N-1\\
		& x(t+1) = A x(t) + b u(t), \ \forall t = 0,\dots,N-1\\
		&x(N) = x_{des}}
Now looking at the nonlinear component, fuel usage as defined by \eqref{eq:fuel_usage_def}, can be equated to:
\begin{equation}
	\begin{aligned}
		\abs{a} \leq g\\
		2 \abs{a} - 1 \leq g\\
	\end{aligned}
\end{equation}
or equivalently,
\begin{equation}
	\begin{aligned}
		-g \leq a \leq g\\
		-g \leq 2a -1 \leq g
	\end{aligned}
\end{equation}

This represents an intersection of two half-spaces which is a simpliler convex restriction.\\
This can now be combined with \eqref{eq:min_fuel_problem_epigraph} to produce the linear program:
\optpblm[eq:min_fuel_problem_result]{F_1 + \cdots + F_{N-1}}{
	-F_t \leq u(t) \leq F_t, \ \forall t = 1, \dots, N-1\\
	&-F_t \leq 2u(t) -1 \leq F_t, \ \forall t = 1, \dots, N-1\\
	& x(t+1) = A x(t) + b u(t), \ \forall t = 0,\dots,N-1\\
	&x(N) = x_{des}}
Which can then be rewritten as:
\optpblm[eq:min_fuel_problem_result]{\vb{1}^T F}{
	-F \leq \vb{u} \leq F\\
	& x(t+1) = A x(t) + b u(t), \ \forall t = 0,\dots,N-1\\
	&x(N) = x_{des}
	}

\newpage
\subsection{Problem 4.28}
Consider the convex quadratic program given as
\begin{equation}\label{eq:convex_quadratic_program}
	\begin{aligned}
		\text{minimize} \ \ & \frac{1}{2} x^T P x + q^T x + r\\
		\text{subject to} \ \ & Ax \leq b
	\end{aligned}
\end{equation}
with a robust equivalent defined as
\begin{equation}\label{eq:robust_convex_quadratic_program}
	\begin{aligned}
		\text{minimize} \ \ & \sup_{P\in \mathcal{E}}\{\frac{1}{2} x^T P x + q^T x + r\}\\
		\text{subject to} \ \ & Ax \leq b
	\end{aligned}
\end{equation}
where $\mathcal{E}$ is the set of all possible matrices of $P$.

\subsubsection{Part a}
\textbf{Problem:}
Express the robust QP as a convex problem given $\mathcal{E} = \{P_1,\dots,P_k\}$ where $P_i\in S^n_+, \ \forall i=1,\dots,k$.\\

\textbf{Solution:}
As a base assumption, by definition all quadratic programs are convex. Additionally when taking a pointwise supremum of convex sets, the result is also convex.
Thus, for a supremum over the finite set of $\mathcal{E}$ it is known that a resultant convex problem can be defined.\\

First, we can redefine the problem as
\optpblm{\frac{1}{2} x^T P x + q^T x + r}{
	P \in\ \mathcal{E}\\
	&Ax \leq b}

This is just wrong....... as in I don't think this es even true...











% no part b,c for 4.28


\newpage
\subsection{Problem 4.43}
Suppose $A: \real^n \to S^m$ is affine such that
\begin{equation}
	A(x) = A_0 + x_1 A_1 + \cdots + x_n A_n
\end{equation}
where $A_i \in S^m$. Let $\lambda_1(x) \geq \lambda_2(x) \geq \cdots \geq \lambda_m(x)$ be the eigenvalues of $A(x)$.\\


For each of the following minimization criteria, formulate the problem as an SDP.\\

\subsubsection{Part a}
\textbf{Problem:}
Minimize the maximum eigenvalue of $A$: $$\text{minimize} \ \ \lambda_1(x)$$
\textbf{Solution:}
It is known that the eigenvalues of a sum of matrices is bounded below by the sum of the minimum eigenvalues of each and bounded above by the sum of the maximum eigenvalues.
\cite{eigvalueBound}
i.e.
$$ \lambda(A)_m + \lambda(B)_m \leq \lambda(A+B)_m \leq \lambda(A+B)_1 \leq \lambda(A)_1 + \lambda(B)_1$$
If this is to be expanded to the entire affine sum, $A(x)$, the objective of minimizing the eigenvalues of the weighted sum of symetric matrices can be done by minimizing the weighted sum of the largest eigenvalues of individual matrices.
This means this problem can be redefined as:
\optpblm{t^T x}{
	t_i = \lambda_1(A_i), \ \forall i = 1,\dots,m\\
	&s = \lambda_1(A_0)}
Since $s$ will remain constant regrdless of $x$, this is equivalent to:
\optpblm{t^T x}{
	t_i = \lambda_1(A_i), \ \forall i = 1,\dots,m}

????????????????????????????????????????????????????
















\newpage
\subsubsection{Part b}
\textbf{Problem:}
Minimize the spread of the eigenvalues of $A$: $$\text{minimize} \ \ \lambda_1(x) - \lambda_m(x)$$\\
\textbf{Solution:}



It is known that the eigenvalues of a sum of matrices is bounded below by the sum of the minimum eigenvalues of each and bounded above by the sum of the maximum eigenvalues.
\cite{eigvalueBound}
i.e.
$$ \lambda(A)_m + \lambda(B)_m \leq \lambda(A+B)_m \leq \lambda(A+B)_1 \leq \lambda(A)_1 + \lambda(B)_1$$
If this is to be expanded to the entire affine sum, $A(x)$, the objective of minimizing the spread eigenvalues of the weighted sum of symetric matrices can be done by minimizing the weighted sum of the spread of eigenvalues of individual matrices.
This means this problem can be redefined as:
\optpblm{t^T x + s}{
	t_i = (\lambda_1(A_i) - \lambda_m(A_i), \ \forall i = 1,\dots,m\\
	&s = (\lambda_1(A_0) - \lambda_m(A_0)}
Since $s$ remains constant regardless of $x$ this is equivelent to
\optpblm{t^T x}{
	t_i = (\lambda_1(A_i) - \lambda_m(A_i), \ \forall i = 1,\dots,m}


????????????????????????????????????????????????????


%Defining the optimization problem as:
%\optpblm{\max \{\lambda\qty(A(x))\} - \min \{\lambda\qty(A(x))\}}{
%	A(x) = A_0 + x_1 A_1 + \cdots + x_n A_n}\\



WHAT???









\subsubsection{Part c}
\textbf{Problem:}
Minimize the conditional number of $A$ while remaining postive definite:
\begin{equation*}
	\begin{aligned}
		\text{minimize} \ \ &k(A(x)) = \frac{\lambda_1(x)}{\lambda_m(x)} \ \forall \ x \in \{x \ | \ A(x) \succ 0\}\\
		 \text{subject to} \ \ &A(x) \succ 0
	\end{aligned}
\end{equation*}
\textbf{Solution:}

















% Only Part a,b,c for 4.43

\newpage
\section{Problem 1: Open-loop optimal control with $1-$ and $\infty-$ norms.}
The following open-loop optimal regulation problem is given as:
\begin{equation}\label{eq:open-loop_opt-control_def}
	\begin{aligned}
		\text{minimze} \hspace{0.5in}
		&\norm{x_T}_p + \sum_{t = 0}^{T-1} \norm{x_t}_p + \gamma\norm{u_t}_q\\
		\text{subject to} \hspace{0.5in}
		& x_{t+1} = A x_t + B u_t, \ t = 0,\dots,T-1\\
		& \norm{x_t}_\infty \leq \bar{x}, \ t = 0,\dots,T\\
		& \norm{u_t}_\infty \leq \bar{u}, \ t = 0,\dots,T
	\end{aligned}
\end{equation}
with $x_t \in \real^n$ and $u_t \in \real^m$ as the system state and control input respectively and parameter $\gamma > 0$ governing the actuator and state regulation performance.\\

\textbf{Problem:}
Express this problem as a linear program for (i) $p=q=\infty$ and (ii) $p=q=1$. Code both in CVX and for the problem data provided. Verify the equivalence between the original optimization problem and transformed linear program obtained and plot the optimal state and input trajectories for each.\\

\textbf{Solution:}
\subsection{Linear program for $p = q = \infty$}

With $p = q = \infty$, the problem is defined as:
\optpblm{\norm{x_T}_\infty + \sum_{t = 0}^{T-1} \norm{x_t}_\infty + \gamma\norm{u_t}_\infty}{
	x_{t+1} = A x_t + B u_t, \ t = 0,\dots,T-1\\
	& \norm{x_t}_\infty \leq \bar{x}, \ t = 0,\dots,T\\
	& \norm{u_t}_\infty \leq \bar{u}, \ t = 0,\dots,T}

The epigraph of this problem can be found as
\optpblm{r_T + (r_0 + \gamma s_0) + (r_{T-1} + \gamma s_{T-1})}{
	\norm{x_t}_\infty \leq r_t, \ t = 0, \dots, T\\
	&\norm{u_i}_\infty \leq s_t, \ t = 0, \dots, T-1\\
	&x_{t+1} = A x_t + B u_t, \ t = 0,\dots,T-1\\
	& \norm{x_t}_\infty \leq \bar{x}, \ t = 0,\dots,T\\
	& \norm{u_t}_\infty \leq \bar{u}, \ t = 0,\dots,T
	}

From the definition of $\norm{x}_\infty = \max \{x\}$ and through vectorization, we can redefine this as the following linear program:
\optpblm{
	\mqty[\vb{1}^T & \gamma \vb{1}^T] \mqty[r\\s] 
	=\vb{1}^T r + \gamma \vb{1}^T s}{
	x_{t+1} = A x_t + B u_t, \ t = 0,\dots,T-1\\
	&x_t \leq r_t \vb{1} \leq \bar{x} \vb{1}, \ t = 0, \dots, T\\
	&u_t \leq s_t \vb{1} \leq \bar{u} \vb{1}, \ t = 0, \dots, T-1
	}

\subsection{Linear program for $p = q = 1$}
With $p = q = 1$, the problem is defined as:
\optpblm{\norm{x_T}_1 + \sum_{t = 0}^{T-1} \norm{x_t}_1 + \gamma\norm{u_t}_1}{
	x_{t+1} = A x_t + B u_t, \ t = 0,\dots,T-1\\
	& \norm{x_t}_\infty \leq \bar{x}, \ t = 0,\dots,T\\
	& \norm{u_t}_\infty \leq \bar{u}, \ t = 0,\dots,T}

The epigraph of this problem can be found as
\optpblm{r_T + (r_0 + \gamma s_0) + (r_{T-1} + \gamma s_{T-1})}{
	\norm{x_t}_1 \leq r_t, \ t = 0, \dots, T\\
	&\norm{u_i}_1 \leq s_t, \ t = 0, \dots, T-1\\
	&x_{t+1} = A x_t + B u_t, \ t = 0,\dots,T-1\\
	& \norm{x_t}_\infty \leq \bar{x}, \ t = 0,\dots,T\\
	& \norm{u_t}_\infty \leq \bar{u}, \ t = 0,\dots,T
}

From the definition of $\norm{x}_1 = \sum_{i=0}^T x$ and through vectorization, we can redefine this as the following linear program:
\optpblm{
	\mqty[\vb{1}^T & \gamma \vb{1}^T] \mqty[r\\s] 
	=\vb{1}^T r + \gamma \vb{1}^T s}{
	x_{t+1} = A x_t + B u_t, \ t = 0,\dots,T-1\\
	& \vb{1}^T x_t \leq r_t, \ t = 0,\dots,T\\
	& \vb{1}^T u_t \leq s_t, \ t = 0,\dots,T-1\\
	&x_t \leq \bar{x} \vb{1}, \ t = 0, \dots, T\\
	&u_t \leq \bar{u} \vb{1}, \ t = 0, \dots, T-1
}

\subsection{CVX Formulation and Results:}

{\Huge Need to put in code explanation still...}





















\newpage
\section{Problem 2: Minimum time state transfer via quasiconvex optimization.}
Consider the LTI system:
\begin{equation}\label{eq:quasiconvex_opt_def}
	\begin{aligned}
		x_{t+1} &= Ax_t + B u_t, \ \forall t = 0,\dots,T\\
		\underline{u} &\leq u_t \leq \bar{u}, \ \forall t = 0,\dots,T
	\end{aligned}
\end{equation}
with $x_0$ as the initial state.\\

\textbf{Problem:}
Show that the minimum time required to transfer the system from $x_0$ to $x_{des}$, given as
\begin{equation}\label{eq:qualiconvex_problem_result}
	f(u_0,\dots,u_T) = \min \{\tau \ | \ x_t = x_{des} \ \text{for} \ \tau \leq t \leq {T+1}\}
\end{equation}
is a quasiconvex function of the control input sequence. Implement a bisection algorithm to solve the problem for the given data.\\

\textbf{Solution:}
Let the following optimization problem be defined:
\optpblm{\tau}{
	x_{t+1} = Ax_t + B u_t, \ \forall t = 0,\dots,T\\
	&\underline{u} \leq u_t \leq \bar{u}, \ \forall t = 0,\dots,T\\
	& x(0) = x_0\\
	& x_t = x_{des} \ \forall \ \tau \leq t \leq T+1
	}
















\newpage
\section{Problem 3: State feedback control design via SDP}
Feedback control problems can be formulated using a semidefinite program, such as
\begin{equation}\label{eq:feedback_control_def}
	\begin{aligned}
		\text{minimze} \hspace{0.5in}& \trace \{P\}\\
		\text{subject to} \hspace{0.5in}
		& \mqty [R + B^T PB & B^T PA\\
				 A^T PB & Q + A^T P A - P] \succeq 0\\
				 & P \succeq 0
	\end{aligned}
\end{equation}
with variable $P \in S^n$ and problem data $A\in \real^{n\cross n}, B \in\real^{n\cross m}, Q \in S^n_+, \real \in S^m_{++}$.\\

This problem is equivalent to the solution to the optimal solution to the infinite-horizon LQR problem:
\begin{equation}\label{eq:LQR_control_def}
	\begin{aligned}
		\text{minimze} \hspace{0.5in}& \sum_{t=0}^\infty x_t^T Q x_t + u_t^T R u_t\\
		\text{subject to} \hspace{0.5in}
		& x_{t+1} = Ax_t + B u_t, \ t \geq 0, \ x(t=0) = x_0
	\end{aligned}
\end{equation}
This is also equivelent to the solution the the discrete-time richotte equation (DARE) and can be solved in matlab with dare(A,B,Q,R). The solution to the feedback controller is
\begin{equation}\label{eq:LQR_control_solution}
	u_t = K x_t\\
	K = -\qty(R + B^T B)^{-1} B^T P^* A
\end{equation}

\textbf{Problem:}
Confirm the solution to the SDP given in \eqref{eq:feedback_control_def} is equivalent to the LQR problem given in \eqref{eq:LQR_control_def} for multiple randomly generated problems.

\textbf{Solution:}
The following results are provided for various randomly generated problems and solutions. This was generated using the code in \appendixname \ref{apx:pblm3_matlab}.

\begin{Verbatim}
	contents....
\end{Verbatim}















\newpage
\appendix
\section{MATLAB Code:}\label{apx:matlab}
All code I write in this course can be found on my GitHub repository:\\
\href{https://github.com/jonaswagner2826/MECH6337}{https://github.com/jonaswagner2826/MECH6327}
\lstinputlisting[caption={MECH6327\_HW3},label={script:HW3}]{MECH6327_HW3.m}

\newpage
\section{Problem 3 MATLAB Code:}\label{apx:pblm1_matlab}
All code I write in this course can be found on my GitHub repository:\\
\href{https://github.com/jonaswagner2826/MECH6327}{https://github.com/jonaswagner2826/MECH6327}
% MECH6313_HW3_pblm1
\lstinputlisting[caption={MECH6327\_HW3\_pblm1},label={script:HW3}]{MECH6327_HW3_pblm1.m}

\newpage
\section{Problem 3 MATLAB Code:}\label{apx:pblm2_matlab}
All code I write in this course can be found on my GitHub repository:\\
\href{https://github.com/jonaswagner2826/MECH6327}{https://github.com/jonaswagner2826/MECH6327}
% MECH6313_HW3_pblm2
\lstinputlisting[caption={MECH6327\_HW3\_pblm2},label={script:HW3}]{MECH6327_HW3_pblm2.m}

\newpage
\section{Problem 3 MATLAB Code:}\label{apx:pblm3_matlab}
All code I write in this course can be found on my GitHub repository:\\
\href{https://github.com/jonaswagner2826/MECH6327}{https://github.com/jonaswagner2826/MECH6327}
% MECH6313_HW3_pblm3
\lstinputlisting[caption={MECH6327\_HW3\_pblm3},label={script:HW3}]{MECH6327_HW3_pblm3.m}












\newpage
\bibliographystyle{ieeetr}
\bibliography{mybib.bib}

\end{document}
